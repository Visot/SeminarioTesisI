\chapter{Resultados}
En este capítulo se discutirán los resultados obtenidos durante el uso de los optimizadores mencionados en el capítulo anterior, con el propósito de acelerar el proceso de entrenamiento de una red neuronal convolucional.\\ Para estas pruebas usamos los datasets CIFAR10 y CIFAR100 que contienen 10 y 100 clases de objetos en su dataset respectivamente. Estos resultados fueron obtenidos usando una computadora con tarjeta gráfica NVIDIA  GTX950M con un total de 640 núcleos y una memoria de 4GB.\\
Nuestra red neuronal convolucional contiene 2 capas de convolución , 2 capas pooling y un fully conected layer( ver Figura A.1 del Apéndice A.1)


\section{Precisión}
En esta sección se mostrarán los resultados de precisión para la tarea de clasificación,usando los dataset CIFAR 10 y CIFAR 100.
\subsection{Precisión en dataset CIFAR-10}
 En la tabla 5.1 podemos ver la precisión para distintos optimizadores(ver Figura A.2 del apéndice A.2), en el cuadro observamos que el método Nesterov al ser una mejora de momentum obtiene una mejor precisión.\\Además se observa que los métodos Adam y RMSprop obtienen los mejores resultados.(ver Figura A.5 del apéndice A.2)
\begin{table}[H]
	\centering
	\caption{Precisión para 5000 epochs}
	\label{my-label}
	\begin{centering}
	\begin{tabular}{@{}lll@{}}
		\toprule
		Método& Precisión($\%$)&  Tiempo de ejecución(s)  \\ \midrule
		Momemtum& 53.04  & 303.07 \\
		Nesterov& 54.09  &  305.59\\
		Adagrad& 44.42 &  310.20 \\
		RMSprop& 68.91 & 322.20 \\
		Adam& 68.02  & 316.51 \\ \bottomrule
	\end{tabular}
	\end{centering}

\end{table}

En el cuadro 5.2 se realizaron 10000 iteraciones en proceso de entrenamiento de nuestra red. Los resultados de la precisión de cada método se muestra este cuadro(ver Figura A.3 del apéndice A.2).\\ El método de ADAM superó al RMSprop que había obteniendo mejores resultados hasta el momento, pero también se pudo observar que el método Adam produjo un resultado ligeramente mejor que el RMSprop pero también empleo un tiempo de ejecución ligeramente mayor.(ver Figura A.5 del apéndice A.2)
\begin{table}[H]
	\centering
	\caption{Precisión para 10000 epochs}
	\label{my-label}
	\begin{centering}
		\begin{tabular}{@{}lll@{}}
			\toprule
			Método& Precisión($\%$)&  Tiempo de ejecución(s)  \\ \midrule
			Momemtum& 60.80  & 585.41\\
			Nesterov& 62.37 &  605.19\\
			Adagrad&47.62  &  598.54 \\
			RMSprop& 70.31 &  621.17\\
			Adam& 70.42 & 622.18 \\ \bottomrule
		\end{tabular}
	\end{centering}
	
\end{table}

\subsection{Precisión en dataset CIFAR-100}
Para obtener más información respecto a los optimizadores se utilizó otro dataset el CIFAR-100 donde obtuvimos los siguientes resultados, (ver Figura A.6 del apéndice A.2) para entrenar la red neural de manera que esta sea capaz de reconocer 100 clases de objetos.\\ En el cuadro 5.3 observamos que los métodos Momentum y Nesterov obtuvieron resultados muy cercanos, pero el método Nesterov lo logra en un menor tiempo. Adagrad obtiene una precisión de 7.73$\%$ la cual fue la más baja de todos los métodos. RMSprop volvió a superar a Adam para este nuevo conjunto de datos, ambos obtuvieron los mejores resultados. (ver Figura A.7 del apéndice A.2) 
\begin{table}[H]
	\centering
	\caption{Precisión para 10000 epochs}
	\label{my-label}
	\begin{centering}
		\begin{tabular}{@{}lll@{}}
			\toprule
			Método& Precisión($\%$)&  Tiempo de ejecución(s)  \\ \midrule
			Momemtum& 17.33  & 597.75\\
			Nesterov& 17.33 &  595.53\\
			Adagrad&7.73  &  628.75 \\
			RMSprop& 36.66 &  622.50\\
			Adam& 35,13 & 624.64 \\ \bottomrule
		\end{tabular}
	\end{centering}
	
\end{table}

\section{Función de costo}

El objetivo principal de utilizar estos métodos es el de optimizar la gradiente de descenso para así reducir la función de costo o perdida. En el programa de utilizo \textit{the cross entropy} como función de pérdida.

\section{Función de costo en CIFAR-10}
El cuadro 5.4 nos muestra los resultados de la reducción de la función de costo mostrando la disminución del error en el dataset CIFAR-10 para 5000 iteraciones(ver Figura A.8 del apéndice A.3).\\ En la figura A.9 del apéndice A.3, podemos observar el comportamiento de la reducción de la función de costo en 50 iteraciones o epochs.\\ La gráfica nos muestra que Adam empieza con un erro alto pero rápidamente logra disminuir y obtener el menor error.\\ \\

La figura A.10 del apéndice A.3, muestra la tendencia final de los métodos de optimización, mostrando que los mejores métodos continúan siendo Adam y RMSprop. 
\begin{table}[H]
	\centering
	\caption{Función costo para 5000 epochs}
	\label{my-label}
	\begin{centering}
		\begin{tabular}{@{}ll@{}}
			\toprule
			Método& Error en función de costo\\ \midrule
			Momemtum& 1.26 \\
			Nesterov& 1.38\\
			Adagrad&1.73   \\
			RMSprop& 0.19 \\
			Adam& 0.30 \\ \bottomrule
		\end{tabular}
	\end{centering}
	
\end{table}

En el cuadro 5.5 mostramos los resultados obtenidos en la reducción del error para 10000 iteraciones usando el CIFAR-10.(ver Figura A.11 del apéndice A.3). En el cuadro se observa que el método Adam obtiene el menor error en comparación a los demás métodos.

\begin{table}[H]
	\centering
	\caption{Función costo para 10000 epochs}
	\label{my-label}
	\begin{centering}
		\begin{tabular}{@{}ll@{}}
			\toprule
			Método& Error en función de costo\\ \midrule
			Momemtum& 1.10 \\
			Nesterov& 1.08\\
			Adagrad&1.64   \\
			RMSprop& 0.19 \\
			Adam& 0.14 \\ 
		\end{tabular}
	\end{centering}
	
\end{table}
\vspace{4cm}
\section{Función de costo en CIFAR-100	}
El cuadro 5.6 muestra los resultados al reducir el error en la función de costo. El cuadro muestrá que Adam y RMSprop tienen los menores errores para clasificar 100 clases de imágenes.
\begin{table}[H]
	\centering
	\caption{Función costo para 10000 epochs}
	\label{my-label}
	\begin{centering}
		\begin{tabular}{@{}ll@{}}
			\toprule
			Método& Error en función de costo\\ \midrule
			Momemtum& 3.46 \\
			Nesterov& 3.48\\
			Adagrad&4.09  \\
			RMSprop& 0.33 \\
			Adam& 0.34 \\ 
		\end{tabular}
	\end{centering}
	
\end{table}