\chapter{Resultados}
En este capítulo se discutirán los resultados obtenidos durante el uso de los optimizadores mencionados en el capítulo anterior para acelerar el proceso de entrenamiento de red neuronal convolucional usando el dataset CIFAR10 y utilizando una computadora con tarjeta gráfica NVIDIA  GTX950M con un total de 640 núcleos y una memoria de 4GB.\\
Estos ejemplos fueron obtenidos al trabajar con una red neuronal convolucional.


\section{Precisión}
En esta sección se muestran los resultados de precisión para la tarea de clasificación, ver apéndice A.1.\\ En la tabla 5.1 podemos ver la precisión para distintos optimizadores, en el cuadro observamos que el método Nesterov al ser una mejora de momentum obtiene una mejor precisión.\\Además se observa que los métodos Adam y RMSprop obtienen los mejores resultados(ver Figura A.4 del apéndice)

\begin{table}[H]
	\centering
	\caption{Precisión para 5000 epochs}
	\label{my-label}
	\begin{centering}
	\begin{tabular}{@{}lll@{}}
		\toprule
		Método& Precisión($\%$)&  Tiempo de ejecución(s)  \\ \midrule
		Momemtum& 52.78  & 273.75 \\
		Nesterov& 54.09  &  271.50\\
		Adagrad& 43.70 &  273.55 \\
		RMSprop& 69.77 & 284.40  \\
		Adam& 69.26  & 338.00 \\ \bottomrule
	\end{tabular}
	\end{centering}

\end{table}
En el cuadro 5.2 se realizaron 10000 iteraciones en proceso de entrenamiento de nuestra red.\\ El método de RMSprop continuó obteniendo mejores resultados, pero también se pudo observar que el método Adam produjo un resultado cercano al RMSprop en un menor tiempo de ejecución.
\begin{table}[H]
	\centering
	\caption{Precisión para 10000 epochs}
	\label{my-label}
	\begin{centering}
		\begin{tabular}{@{}lll@{}}
			\toprule
			Método& Precisión($\%$)&  Tiempo de ejecución(s)  \\ \midrule
			Momemtum& 59.50  & 459.58\\
			Nesterov& 61.12 &  468.56\\
			Adagrad&47.20  &  471.60 \\
			RMSprop& 69.48 &  494.21 \\
			Adam& 68.79 & 483.99 \\ \bottomrule
		\end{tabular}
	\end{centering}
	
\end{table}

\section{Función de costo}

El objetivo principal de utilizar estos métodos fue el de optimizar la gradiente de descenso para así reducir la función de costo o perdida. En el programa de utilizo \textit{the cross entropy} como función de pérdida.
El cuadro 5.2 nos muestra los resultados de la reducción de la función de costo. En la figura A.7 del apéndice A, podemos observar mejor el comportamiento de la reducción de la función de costo en 50 iteraciones o epochs.\\
La figura A.8 muestra la tendencia final de los métodos de optimización, mostrando que los mejores métodos continúan siendo Adam y RMSprop. 
\begin{table}[H]
	\centering
	\caption{Función costo para 5000 epochs}
	\label{my-label}
	\begin{centering}
		\begin{tabular}{@{}ll@{}}
			\toprule
			Método& Función de costo\\ \midrule
			Momemtum& 1.48 \\
			Nesterov& 0.54 \\
			Adagrad&0.43   \\
			RMSprop& 0.27 \\
			Adam& 0.37 \\ \bottomrule
		\end{tabular}
	\end{centering}
	
\end{table}

