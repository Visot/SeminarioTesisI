\documentclass{beamer}
\usepackage{tikz}
\usepackage{smartdiagram}
\usepackage{xmpmulti}
\usepackage{pgfpages}

%\setbeameroption{show notes}
%\setbeameroption{show notes on second screen=right}
\mode<presentation> {
  \usetheme{Warsaw}
  % ou autre ...

  \setbeamercovered{transparent}
  % ou autre chose (il est Ã©galement possible de supprimer cette ligne)
}


\usepackage[french]{babel}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}
\usepackage{tikz}
\pgfdeclareimage[height=1cm]{le-logo}{logouni}
\logo{\pgfuseimage{le-logo}}
\setbeamertemplate{footline}[frame number]


%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title[Patterns for SoS Reconfiguration] 
{Métodos de optimización de la gradiente de
	descenso en una red neuronal convolucional}
%\subtitle {ne complÃ©ter que si l'article possÃ¨de un sous-titre}

\author[Víctor Jesús Sotelo Chico] 
{Víctor Jesús Sotelo Chico\inst{1}}

\institute[]
{
  \inst{1}%
  Universidad Nacional de Ingeniería\\

  }

\date[Seminario de Tesis I] 
{Seminario de Tesis I}



\begin{document}


\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Contenido}
  \tableofcontents
\end{frame}

\section{Introducción}
\begin{frame}{Introducción}
	En la actualidad es indispensable emplear mucho tiempo en el entrenamiento de redes neuronales profundas, por lo que surge la necesidad de encontrar métodos que aceleren este proceso.
\end{frame}
\section{Objetivos}

\begin{frame}{Objetivos}
  %\includegraphics[scale=0.3, angle=-90]{construction-process}
  \begin{itemize}
  	\item Entender las ventajas y desventajas de distintos métodos de optimización de la gradiente de descenso.
	\item Obtener la capacidad de discriminar entre distintos métodos de optimización.
	\item Lograr un mejor entendimiento de las redes neuronales profundas.
  \end{itemize}
\end{frame}


\section{Marco Teórico}
\subsection{Aprendizaje Automático}
\begin{frame}{Aprendizaje Automático}
	Consiste en lograr que las computadoras sean capaces de realizar una predicción o tomar decisiones sin haber estado programada
	explícitamente para realizar esta tarea.\\
	El Aprendizaje Automático puede ser divido de la siguiente forma:
	\begin{itemize}
		\item Aprendizaje Supervisado
		\item Aprendizaje No Supervisado
		\item Aprendizaje por Refuerzo
	\end{itemize}
\end{frame}

\subsection{Redes Neuronales}
\begin{frame}{Redes Neuronales Artificiales}
Estas redes toman como inspiración la arquitectura
del cerebro para la construcción de sistemas inteligente.
Actualmente son la base para el desarrollo de la inteligencia artificial.

\end{frame}
\begin{frame}{Comparación neuronas biológicas y artificiales}
	\begin{figure}
		\centering
		\includegraphics[width=0.6\textwidth]{figures/ANN.png}
		\caption{Redes neuronales biológicas y artificiales }
			\label{neuronas}
	\end{figure} 
\end{frame}

\begin{frame}{Redes neuronales Prealimentadas}
Es un tipo de red neuronal más simple que existe. Esta red puede clasificarse en:

\begin{itemize}
	\item Perceptron simple
	\item Perceptron Multicapas
	\item Redes neuronales convolucionales
\end{itemize}
\end{frame}
\begin{frame}{Esquema Redes neuronales Prealimentadas}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/esquemaff.png}
	\caption{Esquema de Redes Neuronales Prealimentadas }
	\label{neuronasredes}
\end{figure} 
\end{frame}
\begin{frame}{Back Propagation}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.8\textwidth]{figures/backp.png}
		\caption{Propagación hacia atrás}
		\label{backpropagation}
	\end{figure} 
\end{frame}
\begin{frame}{Redes Neuronales Convolucionales(CNN)}
Las CNN son un tipo de redes neuronales especiales para procesar datos
como imágenes. La primera CNN fue creada por Yann LeCun. 

\end{frame}

\begin{frame}{Capas de una red neuronal convolucional}
	\begin{itemize}
		\item Input Layer
		\item Convolutional Layer
		\item Pooling Layer
		\item Fully Conected Layer
		\item Output Layer
	\end{itemize}
	
\end{frame}

\begin{frame}{Input Layer}
Se encarga de recibir la información de entrada.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.5\textwidth]{figures/image.png}
		\caption{Estructura de la imagen de entrada}
	
	\end{figure} 	
\end{frame}

\begin{frame}{Convolutional Layer}
	La capa calculará
	el producto punto entre la región de las neuronas de la capa de entrada y los
	pesos a los que están colocados localmente en la capa de salida. Posee los siguientes hiperparámetros:\\
	\begin{itemize}
		\item Filter size.
		\item Output depth.
		\item Stride.
		\item Zero padding.
	\end{itemize}
\end{frame}

\begin{frame}{Convolutional Layer}


\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{figures/convolucion.jpeg}
	\caption{Operacion de convolución}
	\label{convolucion}
\end{figure}
	
\end{frame}
\begin{frame}{Pooling Layer}
	Se encarga de reducir el tamaño espacial(ancho,alto) de los datos de representación.
		\begin{figure}[H]
			\centering
			\includegraphics[width=0.8\textwidth]{figures/pooling.png}
			\caption{Operación Max pool}

		\end{figure} 
\end{frame}

\begin{frame}{Fully Conected Layer}
	Esta capa será la encargada de reconocer a que clase pertenece una imagen de
	prueba de acuerdo a su puntaje o probabilidad.
	
\end{frame}

\section{Métodos de Optimización}

\begin{frame}{Gradiente de Descenso}
	La gradiente de descenso es una forma de minimizar la función de costo $J(\theta)$ parametrizada por los parámetros $\theta \in\Re^{d}$.
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.6\textwidth]{figures/gd.png}
		\caption{Gradiente de descenso}
		\label{image}
	\end{figure}
\end{frame}
\begin{frame}{Variantes de la Gradiente de Descenso}
	Existen 3 variantes de la gradiente de descenso:
	\begin{itemize}
		\item Batch gradient descent
		\item Stochastic gradient descent
		\item Mini-batch gradient descent
	\end{itemize}

\end{frame}

\begin{frame}{Métodos para optimizar la gradiente de descenso}
	\begin{itemize}
		\item Momentum
		\item Nesterov Momentum 
		\item Adagrad
		\item RMSprop
		\item Adam
	\end{itemize}
\end{frame}
\subsection{Momentum}

\begin{frame}{Momentum}
	El momentum es un método que ayuda a la SGD a acelerar en la dirección
	correcta, mientras evitas las oscilaciones
	\begin{equation}
	\label{mbgds}
	\begin{aligned}
	\nu_{t}&=\gamma \nu_{t-1} +  \eta \nabla_{\theta} J(\theta)\\
	\theta &= \theta -\nu_{t}
	\end{aligned}
	\end{equation}
\end{frame}
\subsection{Nesterov}

\begin{frame}{Nesterov}
	 Esta técnica es una variante de momentum donde se usa el término $\gamma \nu_{t-1}$ para mover los parámetros de $\theta$.
	\begin{equation}
	\label{mbgds}
	\begin{aligned}
	\nu_{t}&=\gamma \nu_{t-1} + \eta \nabla_{\theta} J(\theta- \gamma \nu_{t-1})\\
	\theta &= \theta -\nu_{t}
	\end{aligned}
	\end{equation}
\end{frame}
\subsection{Adagrad}
\begin{frame}{Adagrad}
	Es un algoritmo optimización basado en la gradiente de descenso, del tipo adaptativo.
	\begin{equation}
		\label{adagrad1}
		\begin{aligned}
		g_{t,i}&=\nabla_{\theta} J(\theta_{t,i})\\
		\theta_{t+1,i} &= \theta_{t,i} -\eta \cdot g_{t,i}
		\end{aligned}
	\end{equation}
	
	\begin{equation}
		\label{adagrad2}
		\begin{aligned}
		\theta_{t+1,i} &= \theta_{t,i} - \frac{\eta}{\sqrt{G_{t,ii}+\epsilon}} \cdot g_{t,i}
		\end{aligned}
	\end{equation}
	
	\begin{itemize}
		\item $G_{t,ii}$: suma de los cuadrados de las gradientes pasadas con respecto a $\theta_{i}$
		\item $\epsilon $ es un término pequeño para evitar la división por 0.
	\end{itemize}
\end{frame}
\subsection{RMSprop}
\begin{frame}{RMSprop}
	Método del tipo adaptativo que se desarrollo con el objetivo resolver el problema
	de disminuir radicalmente la tasa de aprendizaje en Adagrad.
	\begin{equation}
		\label{RMS}
		\begin{aligned}
		E[g^2]_{t} &= \gamma E[g^2]_{t-1} + (1-\gamma)g^{2}_{t}\\
		\theta_{t+1} &= \theta_{t} - \frac{\eta}{\sqrt{E[g^2]_{t} +\epsilon }} g_{t}
		\end{aligned}
	\end{equation}
\end{frame}
\subsection{Adam}
\begin{frame}{Adam}
	Adaptative moment estimation o Adam, calcula una
	tasa de aprendizaje adaptativo para cada parámetro.
	\begin{equation}
		\label{adam1}
		\begin{aligned}
		m_{t} &= \beta_{1} m_{t-1} +(1-\beta_{1})g_{t} \\
		v_{t} &= \beta_{2} v_{t-1} +(1-\beta_{2})g_{t}^2
		\end{aligned}
	\end{equation}

	\begin{equation}
	\label{adam2}
	\begin{aligned}
	\hat{m_{t}}&= \frac{m_{t}}{1-\beta_{1}^{t}} \\
	\hat{v_{t}} &= \frac{v_{t}}{1-\beta_{2}^{t}}
	\end{aligned}
	\end{equation}


	\begin{equation}
		\label{adam3}
		\begin{aligned}
		\theta_{t+1}&= \theta_{t+1} - \frac{\eta}{\sqrt{\hat{v_{t}}}+\epsilon} \hat{m_{t}}	
		\end{aligned}
	\end{equation}
\end{frame}

\section{Resultados}
\begin{frame}{Resultados CIFAR-10}
	\begin{figure}
		\centering
		\includegraphics[width=0.6\textwidth]{figures/tablep5000.png}
		\caption{Precisión 50000 epochs }
		\label{precision5000}
	\end{figure} 
\end{frame}
\begin{frame}{Resultados CIFAR-10 - Precisión}
	\begin{figure}[H]
		\includegraphics[width=0.6\textwidth]{figures/optimizadores5000.png}
		\caption{Comparación de precisión para 5000 epochs	}
		\label{some example2}
	\end{figure}
\end{frame}

\begin{frame}{Errores Función de Costo CIFAR-10}
	\begin{figure}[H]
		\includegraphics[width=0.6\textwidth]{figures/tablecost5000.png}
		\caption{Comparación de errores función de costo 5000 epochs	}
		\label{some2}
	\end{figure}
\end{frame}
\begin{frame}{Errores Función de Costo CIFAR-10}
	\begin{figure}[H]
		\includegraphics[width=0.6\textwidth]{figures/optimizadorescross5000050}
		\caption{Comportamiento de errores en la función de costo 5000 epochs	}
		\label{some}
	\end{figure}
\end{frame}

\begin{frame}{Resultados CIFAR-10 - Precisión}
	\begin{figure}
		\centering
		\includegraphics[width=0.6\textwidth]{figures/tablep10000.png}
		\caption{Precisión 100000 epochs }
		\label{precision10000}
	\end{figure} 
\end{frame}
\begin{frame}{Resultados CIFAR-10 - Precisión}
	\begin{figure}[H]
		\includegraphics[width=0.6\textwidth]{figures/optimizadores10000.png}
		\caption{Comparación de precisión para 10000 epochs	}
		\label{some example4}
	\end{figure}
\end{frame}

\begin{frame}{Errores Función de Costo CIFAR-10}
	\begin{figure}[H]
		\includegraphics[width=0.6\textwidth]{figures/tablecost10000.png}
		\caption{Comparación de errores función de costo 10000 epochs	}
		\label{some2}
	\end{figure}
\end{frame}


\begin{frame}{Errores Función de Costo CIFAR-10 }
	\begin{figure}[H]
		\includegraphics[width=0.7\textwidth]{figures/optimizadorescross50000500}
		\caption{Comparación de las errores rango 0-500}
		\label{some6}
		
	\end{figure}
\end{frame}



\begin{frame}{Resultados CIFAR-100 - Precisión}
	\begin{figure}
		\centering
		\includegraphics[width=0.6\textwidth]{figures/tablep10000c100.png}
		\caption{Precisión 100000 epochs }
		\label{precisionc10000}
	\end{figure} 	
\end{frame}

\begin{frame}{Resultados CIFAR-100 - Precisión}
	\begin{figure}[H]
		\includegraphics[width=0.6\textwidth]{figures/cifar100result.png}
		\caption{Comparación de precisión para 10000 epochs	}
		\label{some example43}
	\end{figure}	
\end{frame}

\begin{frame}{Errores Función de Costo CIFAR-100}
	\begin{figure}[H]
		\includegraphics[width=0.7\textwidth]{figures/cifar100error.png}
		\caption{Comparación los errores rango 9900-10000}
		\label{some62}
		
	\end{figure}
\end{frame}
\section{Conclusiones y Trabajos Futuros}

\begin{frame}{Conclusiones}
	\begin{itemize}
		\item Los métodos de optimización Adam y RMSprop obtuvieron los mejores
		resultados de precisión en ambas pruebas.
		\item A pesar de que el método de optimización Adam fue propuesto a partir
		del RMSprop. Adam fue superado en algunas de pruebas realizadas.
		\item Adam es el método que tiene un decaimiento más acelerado al calcular el
		error en la función de costo cross-entropy.

	\end{itemize}
\end{frame}

\begin{frame}{Conclusiones}
	\begin{itemize}
		\item Entre los métodos adaptativos Adam, RMSprop y Adagrad . Solo este
		último obtuvo los peores resultados, esto se debió a su dificultad de
		trabajar con la suma de las gradientes al cuadrado lo cual poco a poco
		redujo su tasa de aprendizaje.
		\item El RMSprop como una mejora del Adagrad, obtuvó mejores resultados
		que este último. Esto debido a que RMSprop trabaja con el promedio
		de la raíz de la gradiente anterior y tasas de decaimiento para controlar
		el problema de la disminución de la tasa de aprendizaje del método
		Adagrad.
	\end{itemize}
\end{frame}

\begin{frame}{Trabajos Futuro}
	\begin{itemize}
		\item Correcto diseño de una red neuronal convolucional.
		\item Obtener resultados con distintos hardwares.
		\item Realizar una implementación más interactiva.
	\end{itemize}
\end{frame}

\end{document}